{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNVafNCxYuHQw+6SHMteiC5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Introduction to Keras using a TensorFlow 2.0 Backend**\n","\n","1. Loading our MNIST dataset\n","2. Inspecting our dataset\n","3. Visualizing our image dataset\n","5. Preprocessing our dataset\n","6. Building our Model\n","7. Training our Model\n","8. Plotting our training logs\n","9. Saving and Loading our Model\n","10. Testing our model on test data"],"metadata":{"id":"FN0zAC2HMUbg"}},{"cell_type":"markdown","source":["## **1. Loading our Data**\n","\n","There are built in datasets from ```tensorflow.keras.datasets``` to load our data. We use the ```mnist.load_data()``` function.\n","\n","Returns: **2 tuples**\n","- x_train, x_test: uint8 array of RGB image data with shape (num_samples, 3, 32, 32) or (num_samples, 32, 32, 3) based on the image_data_format backend setting of either channels_first or channels_last respectively.\n","- y_train, y_test: uint8 array of category labels (integers in range 0-9) with shape (num_samples, 1).\n","\n","- More info on available datases at https://keras.io/datasets/"],"metadata":{"id":"n-mSs_WBMeOi"}},{"cell_type":"code","source":["# We can load the built in datasets from this function\n","from tensorflow.keras.datasets import mnist\n","\n","# loads the MNIST training and test dataset\n","(x_train, y_train), (x_test, y_test)  = mnist.load_data()"],"metadata":{"id":"7eKRfvOqMU5J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **A quick check to see if we're using the GPU**"],"metadata":{"id":"1RGjxzyFMy_w"}},{"cell_type":"code","source":["# Check to see if we're using the GPU\n","from tensorflow.python.client import device_lib\n","\n","print(device_lib.list_local_devices())"],"metadata":{"id":"nr7nm0F_MjzH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2. Inspecting our dataset**"],"metadata":{"id":"IAGk9u3mM_2N"}},{"cell_type":"code","source":["# Display the number of samples in x_train, x_test, y_train, y_test\n","print(\"Initial shape or dimensions of x_train\", str(x_train.shape))\n","\n","# Print the number of samples in our data\n","print (\"Number of samples in our training data: \" + str(len(x_train)))\n","print (\"Number of labels in our training data: \" + str(len(y_train)))\n","print (\"Number of samples in our test data: \" + str(len(x_test)))\n","print (\"Number of labels in our test data: \" + str(len(y_test)))\n","\n","# Print the image dimensions and no. of labels in our Training and Test Data\n","print(\"\\n\")\n","print (\"Dimensions of x_train:\" + str(x_train[0].shape))\n","print (\"Labels in x_train:\" + str(y_train.shape))\n","print(\"\\n\")\n","print (\"Dimensions of x_test:\" + str(x_test[0].shape))\n","print (\"Labels in y_test:\" + str(y_test.shape))"],"metadata":{"id":"xIPHOtWqM1fY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3. Visualizing our image dataset**\n","\n","Let's take a look at some of images in this dataset\n","- Using Matplotlib"],"metadata":{"id":"OqFJdIIkNGpw"}},{"cell_type":"code","source":["# Let's view the 50 first images of the MNIST training dataset\n","import matplotlib.pyplot as plt\n","\n","# Create figure and change size\n","figure = plt.figure()\n","plt.figure(figsize=(16,10))\n","\n","# Set how many images we wish to see\n","num_of_images = 50\n","\n","# iterate index from 1 to 51\n","for index in range(1, num_of_images + 1):\n","    plt.subplot(5, 10, index).set_title(f'{y_train[index]}')\n","    plt.axis('off')\n","    plt.imshow(x_train[index], cmap='gray_r')"],"metadata":{"id":"LTOul8ziNCPy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **4. Preprocessing our dataset**\n","\n","Before passing our data to our CNN for training, we need to prepare it firstly. This entials:\n","1. Reshaping our data by adding a 4th Dimension\n","2. Changing our datatype from uint8 to float32\n","3. Normalizing our data to values between 0 and 1\n","4. One hot encoding"],"metadata":{"id":"t33KvDoXNZue"}},{"cell_type":"code","source":["# Lets store the number of rows and columns\n","img_rows = x_train[0].shape[0]\n","img_cols = x_train[0].shape[1]\n","\n","# Getting our data in the right 'shape' needed for Keras\n","# We need to add a 4th dimenion to our data thereby changing our\n","# Our original image shape of (60000,28,28) to (60000,28,28,1)\n","x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","\n","# store the shape of a single image\n","input_shape = (img_rows, img_cols, 1)\n","\n","# change our image type to float32 data type\n","x_train = x_train.astype('float32') #uint8 originally\n","x_test = x_test.astype('float32')\n","\n","# Normalize our data by changing the range from (0 to 255) to (0 to 1)\n","x_train /= 255.0\n","x_test /= 255.0\n","\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')"],"metadata":{"id":"-bl95tXUNQro"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **One Hot Encode Our Labels**\n","\n","We can easily implement this transformm using ```to_categorical``` from ``` tensorflow.keras.utils```"],"metadata":{"id":"FIz7df3PNXLT"}},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","\n","# Now we one hot encode outputs\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","# Let's count the number columns in our hot encoded matrix\n","print (\"Number of Classes: \" + str(y_test.shape[1]))\n","\n","num_classes = y_test.shape[1]\n","num_pixels = x_train.shape[1] * x_train.shape[2]"],"metadata":{"id":"3rbd1RZuOJZd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train[0]"],"metadata":{"id":"HPsHFBa-OSuV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **5. Building Our Model**\n","\n","![](https://github.com/rajeevratan84/ModernComputerVision/raw/main/CleanShot%202020-11-29%20at%204.21.04%402x.png)\n","- We're constructing a simple but effective CNN that uses 32 filters of size 3x3\n","- We've added a 2nd CONV layer of 64 filters of the same size 3x3\n","- We then downsample our data to 2x2\n","- We then flatten our Max Pool output that is connected to a Dense/FC layer that has an output size of 128\n","- Then we connect our 128 outputs to another FC/Dense layer that outputs to the 10 categorical units"],"metadata":{"id":"N1YNnf9cOgpP"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.optimizers import SGD\n","\n","# create model\n","model = Sequential()\n","\n","# Our First Convolution Layer, Filter size 32 which reduces our layer size to 26 x 26 x 32\n","# We use ReLU activation and specify our input_shape which is 28 x 28 x 1\n","model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n","\n","# Our Second Convolution Layer, Filter size 64 which reduces our layer size to 24 x 24 x 64\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","\n","# We use MaxPooling with a kernel size of 2 x 2, this reduces our size to 12 x 12 x 64\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","# We then Flatten our tensor object before input into our Dense Layer\n","# A flatten operation on a tensor reshapes the tensor to have the shape that is\n","# equal to the number of elements contained in tensor\n","# In our CNN it goes from 12 * 12 * 64 to 9216 * 1\n","model.add(Flatten())\n","\n","# We connect this layer to a Fully Connected/Dense layer of size 1 * 128\n","model.add(Dense(128, activation='relu'))\n","\n","# We create our final Fully Connected/Dense layer with an output for each class (10)\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# We compile our model, this creates an object that stores the model we just created\n","# We set our Optimizer to use Stochastic Gradient Descent (learning rate of 0.001)\n","# We set our loss function to be categorical_crossentropy as it's suitable for multiclass problems\n","# Finally, the metrics (What we judge our performance on) to be accuracy\n","model.compile(loss = 'categorical_crossentropy',\n","              optimizer = SGD(0.001),\n","              metrics = ['accuracy'])\n","\n","# We can use the summary function to display our model layers and parameters\n","print(model.summary())"],"metadata":{"id":"uMxI4lTpOb61"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **6. Training our Model**\n","- Our preprocessed data is used as the input\n","- We set the batch size to 128 (or any number ranging from 8 to 256 is good)\n","- We set the number of epochs to 2, this is just for this tutorial purpose, but a value for at least 10 should be used\n","- We store our model's training results for plotting in future\n","- We then use Kera's molel.evaluate function to output the model's final performance. Here we are examing Test Loss and Test Accuracy"],"metadata":{"id":"PYQC9RNbPxQs"}},{"cell_type":"code","source":["batch_size = 128\n","epochs = 25\n","\n","# Store our results here so we can plot later\n","# In our fit function we specify our datsets (x_train & y_train),\n","# the batch size (typically 16 to 128 depending on your RAM), the number of\n","# epochs (usually 10 to 100) and our validation datasets (x_test & y_test)\n","# verbose = 1, sets our training to output performance metrics every epoch\n","history = model.fit(x_train,\n","                    y_train,\n","                    batch_size = batch_size,\n","                    epochs = epochs,\n","                    verbose = 1,\n","                    validation_data = (x_test, y_test))"],"metadata":{"id":"6APw-wQAPeqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We obtain our accuracy score using the evalute function\n","# Score holds two values, our Test loss and Accuracy\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"metadata":{"id":"QBHY9I9FP_tS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **7. Ploting our Loss and Accuracy Charts**"],"metadata":{"id":"DZBR8z5ZQ_O6"}},{"cell_type":"code","source":["history_dict = history.history\n","history_dict"],"metadata":{"id":"GbYe-_zTQm7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting our loss charts\n","import matplotlib.pyplot as plt\n","\n","# Use the History object we created to get our saved performance results\n","history_dict = history.history\n","\n","# Extract the loss and validation losses\n","loss_values = history_dict['loss']\n","val_loss_values = history_dict['val_loss']\n","\n","# Get the number of epochs and create an array up to that number using range()\n","epochs = range(1, len(loss_values) + 1)\n","\n","# Plot line charts for both Validation and Training Loss\n","line1 = plt.plot(epochs, val_loss_values, label='Validation/Test Loss')\n","line2 = plt.plot(epochs, loss_values, label='Training Loss')\n","plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n","plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.grid(True)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"r8CdicM2RC26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plotting our accuracy charts\n","import matplotlib.pyplot as plt\n","\n","history_dict = history.history\n","\n","acc_values = history_dict['accuracy']\n","val_acc_values = history_dict['val_accuracy']\n","epochs = range(1, len(loss_values) + 1)\n","\n","line1 = plt.plot(epochs, val_acc_values, label='Validation/Test Accuracy')\n","line2 = plt.plot(epochs, acc_values, label='Training Accuracy')\n","plt.setp(line1, linewidth=2.0, marker = '+', markersize=10.0)\n","plt.setp(line2, linewidth=2.0, marker = '4', markersize=10.0)\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.grid(True)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Ty_2S18LRFdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **8. Saving and Loadng our Model**\n","\n","**Saving our Model is simple, just use:**\n","\n"," ```model.save(\"model_file_name.h5\")```"],"metadata":{"id":"ssAwLx8DRW6W"}},{"cell_type":"code","source":["model.save(\"mnist_simple_cnn_10_Epochs.h5\")\n","print(\"Model Saved\")"],"metadata":{"id":"km61eIwgRQq6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Loading our Saved Model is also simple, just use:**\n","\n","```load_model(model_file_name.h5)```"],"metadata":{"id":"wzLp8P1jRfVv"}},{"cell_type":"code","source":["# We need to import our load_model function\n","from tensorflow.keras.models import load_model\n","\n","classifier = load_model('mnist_simple_cnn_10_Epochs.h5')"],"metadata":{"id":"P2ljRGu5RbrP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##  **9. Getting Predictions from our sample Test Data**\n","\n","**Predicting all test data**"],"metadata":{"id":"VLMBs0fQRkg0"}},{"cell_type":"code","source":["import numpy as np\n","\n","#x_test = x_test.reshape(10000,28,28,1)\n","print(x_test.shape)\n","\n","print(\"Predicting classes for all 10,000 test images...\")\n","\n","pred = np.argmax(classifier.predict(x_test), axis=-1)\n","print(\"Completed.\\n\")\n","\n","print(pred)\n","print(type(pred))\n","print(len(pred))"],"metadata":{"id":"b_SQwYjBRhhL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["(x_train, y_train), (x_test, y_test)  = mnist.load_data()\n","\n","def imgshow(title=\"\", image = None, size = 6):\n","    w, h = image.shape[0], image.shape[1]\n","    aspect_ratio = w/h\n","    plt.figure(figsize=(size * aspect_ratio,size))\n","    plt.imshow(image, cmap='gray_r')\n","    plt.title(title)\n","    plt.show()\n","\n","for i in range(0,10):\n","    rand = np.random.randint(0,len(x_test))\n","    img = x_test[rand] # (28, 28)\n","    img_reshape = img.reshape(1,28,28,1)\n","    pred = np.argmax(classifier.predict(img_reshape, verbose=0), axis=-1)\n","    label = y_test[rand]\n","    print(f'Actual Label: {label}, Predicted Label: {pred}')\n","    imgshow(\"\", img, size = 1)"],"metadata":{"id":"xXpVQv-0RsrA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XVTG2y-ES_X7"},"execution_count":null,"outputs":[]}]}