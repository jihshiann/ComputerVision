# -*- coding: utf-8 -*-
"""homework_2_20231221.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y11JxOZAB74O0gvigWSTW0VZFRb9o8KC
"""

!pip install kaggle

import tensorflow as tf

if tf.test.gpu_device_name():
    print("GPU : {}".format(tf.test.gpu_device_name()))
else:
    print("NO")

"""## Load data
You can use the API from kaggle or download the data set from kaggle

https://www.kaggle.com/datasets/sovitrath/cub-200-bird-species-xml-detection-dataset/code?datasetId=2795899

"""

from google.colab import files

upload kaggle.json
uploaded = files.upload()


!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d sovitrath/cub-200-bird-species-xml-detection-dataset

# 解壓縮
import zipfile
with zipfile.ZipFile('cub-200-bird-species-xml-detection-dataset.zip', 'r') as zip_ref:
    zip_ref.extractall("cub-200-bird-species-xml-detection-dataset")

#trainImg_folder = '/content/cub-200-bird-species-xml-detection-dataset/cub_200_2011_xml/train_images'
#trainLabel_folder = '/content/cub-200-bird-species-xml-detection-dataset/cub_200_2011_xml/train_labels'
#validImg_folder = '/content/cub-200-bird-species-xml-detection-dataset/cub_200_2011_xml/valid_images'
#validLabel_folder = '/content/cub-200-bird-species-xml-detection-dataset/cub_200_2011_xml/valid_labels'

trainImg_folder = '/kaggle/input/cub-200-bird-species-xml-detection-dataset/cub_200_2011_xml/train_images'
trainLabel_folder = '/kaggle/input/cub-200-bird-species-xml-detection-dataset/cub_200_2011_xml/train_labels'
validImg_folder = '/kaggle/input/cub-200-bird-species-xml-detection-dataset/cub_200_2011_xml/valid_images'
validLabel_folder = '/kaggle/input/cub-200-bird-species-xml-detection-dataset/cub_200_2011_xml/valid_labels'

"""##Choosing Model

Choose an object detection model suitable for detecting birds, such as YOLO, SSD, Faster R-CNN, etc. You can choose to use a pre-trained model or train a new model, depending on your dataset and task requirements.
"""

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, Reshape
from tensorflow.keras.layers import Conv2D, Reshape, Concatenate

def create_ssd_model():
    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    # 凍結基礎模型的層
    for layer in base_model.layers:
        layer.trainable = False

    # 添加新層来预测边界框
    x = base_model.output
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(4)(x)  # 只有4个单元，对应一个边界框的(xmin, ymin, xmax, ymax)

    model = Model(inputs=base_model.input, outputs=x)
    return model

ssd_model = create_ssd_model()

ssd_model.summary()

"""## Preprocessing and Training (optional) (1%)

Read the downloaded image data using an appropriate image processing library (e.g. OpenCV, PIL). Next, the XML annotation file is parsed to obtain the location and category information of the bird objects in each image. The images are then scaled and normalized according to the needs of the model, ensuring that the dimensions and numerical range of the input image match the model's expectations. Finally, the data set is divided into a training set and a validation set to evaluate the model performance when training the model.

If you choose to train the model, please choose an appropriate loss function (such as mean square error), an optimizer (such as Adam or SGD), and the corresponding training parameters, such as learning rate and batch size. Model training is done using the training set, either transfer learning (initialized with pre-trained model weights) or training from scratch. Finally, the performance of the model is evaluated using the validation set to ensure its generalization ability.
"""

import tensorflow as tf
from tensorflow.keras.utils import Sequence
import os
import cv2
import numpy as np
import xml.etree.ElementTree as ET

def resize_box(box, orig_size, target_size):
    x_scale = target_size[0] / orig_size[0]
    y_scale = target_size[1] / orig_size[1]

    resized_box = [
        int(box[0] * x_scale),
        int(box[1] * y_scale),
        int(box[2] * x_scale),
        int(box[3] * y_scale)
    ]
    return resized_box

class BirdDataset(Sequence):
    def __init__(self, image_folder, label_folder, batch_size, img_size=(224, 224)):
        self.image_folder = image_folder
        self.label_folder = label_folder
        self.batch_size = batch_size
        self.img_size = img_size
        self.image_filenames = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]

    def __len__(self):
        return int(np.ceil(len(self.image_filenames) / float(self.batch_size)))

    def __getitem__(self, idx):
        batch_x = self.image_filenames[idx * self.batch_size:(idx + 1) * self.batch_size]
        images = []
        labels = []

        for file_name in batch_x:
            img_path = os.path.join(self.image_folder, file_name)  # 加載圖像的路徑
            image = self.load_image(img_path)  # 加載並處理圖像
            images.append(image)  # 添加處理後的圖像到列表

            label_path = os.path.join(self.label_folder, file_name.replace('.jpg', '.xml'))
            label = self.parse_xml(label_path)
            labels.append(label)

        return np.array(images), np.array(labels)

    def load_image(self, path):
        image = cv2.imread(path)
        image = cv2.resize(image, self.img_size)
        image = image / 255.0  # Normalize to [0, 1]
        return image

    def parse_xml(self, xml_file):
        tree = ET.parse(xml_file)
        root = tree.getroot()

        # 获取原始图像尺寸
        for size in root.findall('size'):
            orig_width = int(size.find('width').text)
            orig_height = int(size.find('height').text)

        orig_size = (orig_width, orig_height)
        target_size = self.img_size

        # 假设每个图片中只有一个对象
        member = root.find('object')
        bndbox = member.find('bndbox')
        xmin = int(bndbox.find('xmin').text)
        ymin = int(bndbox.find('ymin').text)
        xmax = int(bndbox.find('xmax').text)
        ymax = int(bndbox.find('ymax').text)

        box = [xmin, ymin, xmax, ymax]
        resized_box = resize_box(box, orig_size, target_size)

        return resized_box  # 返回单个边界框

for images, labels in valid_dataset:
    print("Images shape:", images.shape)
    print("Labels shape:", labels.shape)
    break

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError, CategoricalCrossentropy

# 設定學習率
learning_rate = 0.001

# 使用Adam優化器，並設定學習率
adam_optimizer = Adam(learning_rate=learning_rate)


# 使用數據生成器
batch_size = 32  # 根據您的記憶體限制選擇一個合適的數值
train_dataset = BirdDataset(trainImg_folder, trainLabel_folder, batch_size)
valid_dataset = BirdDataset(validImg_folder, validLabel_folder, batch_size)


# 在編譯模型時使用自定義損失函數
ssd_model.compile(optimizer=adam_optimizer, loss='mean_squared_error')


# 訓練模型
epochs = 10 # 可以調整訓練的輪次
ssd_model.fit(train_dataset, epochs=epochs, validation_data=valid_dataset)

"""## Evaluate Model Performance (1%)

Use the trained model to detect objects on the validation set, and calculate evaluation indicators such as average precision (AP) to evaluate the performance of the model. This helps confirm the model's accuracy and generalization ability on new data

"""

# 評估模型
performance = ssd_model.evaluate(valid_dataset)
print("模型性能:", performance)

"""## Inference and Visualization (3%)
For 5 test images, the trained model is used for inference, showing the predicted bounding boxes and comparing with the actual annotated ground truth. This helps visualize how the model performs in real-world situations and ensures it correctly detects and localizes avian objects.
"""

import matplotlib.pyplot as plt
import matplotlib.patches as patches

def visualize_predictions(image, predictions, ground_truth):
    fig, ax = plt.subplots(1)
    ax.imshow(image)

    # 直接對預測的邊界框應用NMS
    selected_boxes = non_max_suppression(predictions, iou_threshold=0.5)

    # 繪製經過NMS處理後的預測邊界框
    for box in selected_boxes:
        rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=2, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

    # 繪製實際的邊界框
    for truth in ground_truth:
        rect = patches.Rectangle((truth[0], truth[1]), truth[2] - truth[0], truth[3] - truth[1], linewidth=2, edgecolor='g', facecolor='none')
        ax.add_patch(rect)

    plt.show()


# 選擇5張測試圖像進行推理和可視化
num_images_to_visualize = 5
count = 0

for batch_images, batch_labels in valid_dataset:
    predictions = ssd_model.predict(batch_images)

    for i in range(len(batch_images)):
        # 注意：預測的輸出是邊界框坐標，不再需要分離出邊界框和置信度
        visualize_predictions(batch_images[i], predictions[i], batch_labels[i])
        count += 1

        if count >= num_images_to_visualize:
            break

    if count >= num_images_to_visualize:
        break

import numpy as np

def non_max_suppression(boxes, iou_threshold=0.01):
    if len(boxes) == 0:
        return []

    # 将框的坐标转换为浮点数（如果它们不是）
    if boxes.dtype.kind == "i":
        boxes = boxes.astype("float")

    # 初始化选中框的列表
    pick = []

    # 获取边界框坐标
    x1 = boxes[:,0]
    y1 = boxes[:,1]
    x2 = boxes[:,2]
    y2 = boxes[:,3]

    # 计算边界框面积并排序
    area = (x2 - x1) * (y2 - y1)
    idxs = np.argsort(y2)

    while len(idxs) > 0:
        last = len(idxs) - 1
        i = idxs[last]
        pick.append(i)

        # 找到剩余框与当前框的最大交集坐标
        xx1 = np.maximum(x1[i], x1[idxs[:last]])
        yy1 = np.maximum(y1[i], y1[idxs[:last]])
        xx2 = np.minimum(x2[i], x2[idxs[:last]])
        yy2 = np.minimum(y2[i], y2[idxs[:last]])

        # 计算宽度和高度，以及交集面积
        w = np.maximum(0, xx2 - xx1)
        h = np.maximum(0, yy2 - yy1)
        overlap = (w * h) / area[idxs[:last]]

        # 删除所有与当前框重叠度高于阈值的框
        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > iou_threshold)[0])))

    # 返回最终选中的边界框
    return boxes[pick].astype("int")




def iou(box1, box2):
    # 計算交集區域
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    intersection_area = max(0, x2 - x1) * max(0, y2 - y1)

    # 計算各自區域
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])

    # 計算並集區域
    union_area = box1_area + box2_area - intersection_area

    # 計算交並比
    return intersection_area / union_area if union_area != 0 else 0

import matplotlib.pyplot as plt
import matplotlib.patches as patches

def visualize_predictions(image, prediction, ground_truth):
    fig, ax = plt.subplots(1)
    ax.imshow(image)

    # 画出预测的边界框
    rect = patches.Rectangle((prediction[0], prediction[1]), prediction[2] - prediction[0], prediction[3] - prediction[1], linewidth=2, edgecolor='r', facecolor='none')
    ax.add_patch(rect)

    # 检查ground_truth是否是二维数组
    if ground_truth.ndim == 2:
        for truth in ground_truth:
            rect = patches.Rectangle((truth[0], truth[1]), truth[2] - truth[0], truth[3] - truth[1], linewidth=2, edgecolor='g', facecolor='none')
            ax.add_patch(rect)
    elif ground_truth.ndim == 1:
        rect = patches.Rectangle((ground_truth[0], ground_truth[1]), ground_truth[2] - ground_truth[0], ground_truth[3] - ground_truth[1], linewidth=2, edgecolor='g', facecolor='none')
        ax.add_patch(rect)

    plt.show()

num_images_to_visualize = 10
count = 0

for batch_images, batch_labels in valid_dataset:
    predictions = ssd_model.predict(batch_images)

    for i in range(len(batch_images)):
        # batch_labels[i] 應該包含真實標籤的邊界框
        # predictions[i] 應該包含預測的邊界框
        # 如果 batch_labels 或 predictions 不是 NumPy 陣列，您可能需要將它們轉換過來
        visualize_predictions(batch_images[i], predictions[i], batch_labels[i])
        count += 1

        if count >= num_images_to_visualize:
            break

    if count >= num_images_to_visualize:
        break